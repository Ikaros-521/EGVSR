import os
import sys
import torch
import numpy as np
import cv2
import gradio as gr
import yaml
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('codes')

from models import define_model
from utils import data_utils, base_utils


class FixedVSRApp:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.model_name = None
        
    def load_model(self, model_name: str):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        if self.model_name == model_name and self.model is not None:
            return self.model
            
        # æ¨¡å‹é…ç½®
        model_configs = {
            'EGVSR': {
                'model_name': 'tecogan',
                'config_file': 'experiments_BD/EGVSR/001/test.yml',
                'checkpoint': 'pretrained_models/EGVSR_iter420000.pth'
            },
            'TecoGAN': {
                'model_name': 'tecogan', 
                'config_file': 'experiments_BD/TecoGAN/001/test.yml',
                'checkpoint': 'pretrained_models/TecoGAN_BD_iter500000.pth'
            },
            'FRVSR': {
                'model_name': 'frvsr',
                'config_file': 'experiments_BD/FRVSR/001/test.yml', 
                'checkpoint': 'pretrained_models/FRVSR_BD_iter400000.pth'
            }
        }
        
        if model_name not in model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            
        config = model_configs[model_name]
        
        try:
            # è¯»å–é…ç½®æ–‡ä»¶
            with open(config['config_file'], 'r', encoding='utf-8') as f:
                opt = yaml.load(f.read(), Loader=yaml.FullLoader)
            
            # è®¾ç½®æ¨¡å‹å‚æ•°
            opt['model']['name'] = config['model_name']
            opt['model']['generator']['load_path'] = config['checkpoint']
            opt['device'] = str(self.device)
            opt['is_train'] = False
            opt['verbose'] = False
            
            # åˆ›å»ºæ¨¡å‹
            self.model = define_model(opt)
            self.model_name = model_name
            
            return self.model
            
        except Exception as e:
            raise ValueError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
    
    def extract_frames(self, video_path: str, max_frames: int = 20) -> tuple:
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        frames = []
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        frame_count = 0
        while len(frames) < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
                
            # è½¬æ¢ä¸ºRGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
            frame_count += 1
            
        cap.release()
        
        return frames, fps
    
    def create_low_resolution_frames(self, frames: list, scale: int = 4, method: str = 'bicubic') -> list:
        """åˆ›å»ºä½åˆ†è¾¨ç‡å¸§"""
        lr_frames = []
        for frame in frames:
            h, w = frame.shape[:2]
            lr_h, lr_w = h // scale, w // scale
            
            if method == 'bicubic':
                lr_frame = cv2.resize(frame, (lr_w, lr_h), interpolation=cv2.INTER_CUBIC)
            elif method == 'bilinear':
                lr_frame = cv2.resize(frame, (lr_w, lr_h), interpolation=cv2.INTER_LINEAR)
            elif method == 'nearest':
                lr_frame = cv2.resize(frame, (lr_w, lr_h), interpolation=cv2.INTER_NEAREST)
            else:
                lr_frame = cv2.resize(frame, (lr_w, lr_h), interpolation=cv2.INTER_CUBIC)
                
            lr_frames.append(lr_frame)
        return lr_frames
    
    def process_video_fixed(self, video_path: str, model_name: str, max_frames: int = 20, 
                           create_lr: bool = True, downscale_method: str = 'bicubic'):
        """ä¿®å¤ç‰ˆæœ¬çš„è§†é¢‘å¤„ç†"""
        try:
            # åŠ è½½æ¨¡å‹
            model = self.load_model(model_name)
            
            # æå–å¸§
            frames, fps = self.extract_frames(video_path, max_frames)
            if not frames:
                raise ValueError("æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§")
            
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šæ˜¯å¦åˆ›å»ºä½åˆ†è¾¨ç‡å¸§
            if create_lr:
                # åˆ›å»ºä½åˆ†è¾¨ç‡å¸§ç”¨äºæ¼”ç¤º
                lr_frames = self.create_low_resolution_frames(frames, method=downscale_method)
                input_frames = lr_frames
                original_frames = frames  # ä¿å­˜åŸå§‹é«˜åˆ†è¾¨ç‡å¸§ç”¨äºå¯¹æ¯”
            else:
                # ç›´æ¥ä½¿ç”¨åŸå§‹å¸§ä½œä¸ºè¾“å…¥
                input_frames = frames
                original_frames = None
            
            # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ•°æ®å¤„ç†æµç¨‹
            # 1. å †å å¸§
            stacked_frames = np.stack(input_frames)  # (T, H, W, C)
            
            # 2. ä½¿ç”¨é¡¹ç›®çš„æ•°æ®å¤„ç†å·¥å…·è¿›è¡Œæ­£ç¡®çš„æ•°æ®è½¬æ¢
            input_tensor = data_utils.canonicalize(stacked_frames)  # å½’ä¸€åŒ–åˆ°[0,1]
            
            print(f"è¾“å…¥tensorå½¢çŠ¶: {input_tensor.shape}")
            print(f"è¾“å…¥tensorèŒƒå›´: [{input_tensor.min():.3f}, {input_tensor.max():.3f}]")
            
            # 3. æ¨ç†
            with torch.no_grad():
                # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
                if hasattr(model, 'net_G'):
                    model.net_G.eval()
                sr_frames = model.infer(input_tensor)
            
            # 4. å¤„ç†è¾“å‡º - æ¨¡å‹è¾“å‡ºå·²ç»æ˜¯uint8æ ¼å¼
            if isinstance(sr_frames, torch.Tensor):
                sr_frames = sr_frames.cpu().numpy()
            
            print(f"è¾“å‡ºå½¢çŠ¶: {sr_frames.shape}")
            print(f"è¾“å‡ºèŒƒå›´: [{sr_frames.min()}, {sr_frames.max()}]")
            
            # 5. ç¡®ä¿è¾“å‡ºæ˜¯uint8æ ¼å¼
            sr_frames = np.clip(sr_frames, 0, 255).astype(np.uint8)
            sr_frames_list = [frame for frame in sr_frames]
            
            # 6. åˆ›å»ºè¾“å‡ºè§†é¢‘
            output_path = video_path.replace('.mp4', '_sr.mp4')
            h, w = sr_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))
            
            for frame in sr_frames:
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                out.write(frame_bgr)
                
            out.release()
            
            # è¿”å›ç»“æœ
            if create_lr:
                return output_path, lr_frames, sr_frames_list, original_frames
            else:
                return output_path, frames, sr_frames_list, None
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            raise gr.Error(f"å¤„ç†è§†é¢‘æ—¶å‡ºé”™: {str(e)}")


def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    app = FixedVSRApp()
    
    with gr.Blocks(title="ä¿®å¤ç‰ˆè§†é¢‘è¶…åˆ†è¾¨ç‡ç³»ç»Ÿ") as interface:
        gr.Markdown("# ğŸ¬ ä¿®å¤ç‰ˆè§†é¢‘è¶…åˆ†è¾¨ç‡ç³»ç»Ÿ")
        gr.Markdown("ä¿®å¤äº†è¶…åˆ†ç»“æœåªæœ‰è½®å»“çš„é—®é¢˜")
        
        with gr.Row():
            with gr.Column():
                video_input = gr.Video(label="è¾“å…¥è§†é¢‘")
                model_selector = gr.Dropdown(
                    choices=['EGVSR', 'TecoGAN', 'FRVSR'],
                    value="EGVSR",
                    label="é€‰æ‹©æ¨¡å‹"
                )
                max_frames = gr.Slider(
                    minimum=5,
                    maximum=30,
                    value=20,
                    step=5,
                    label="æœ€å¤§å¤„ç†å¸§æ•°"
                )
                
                # æ·»åŠ æ–°çš„é€‰é¡¹
                with gr.Row():
                    create_lr_checkbox = gr.Checkbox(
                        value=True,
                        label="åˆ›å»ºä½åˆ†è¾¨ç‡è¾“å…¥ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"
                    )
                    downscale_method = gr.Dropdown(
                        choices=['bicubic', 'bilinear', 'nearest'],
                        value='bicubic',
                        label="ä¸‹é‡‡æ ·æ–¹æ³•"
                    )
                
                process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
            
            with gr.Column():
                video_output = gr.Video(label="è¶…åˆ†è¾¨ç‡ç»“æœ")
                status_text = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
        
        # å¸§å¯¹æ¯”
        with gr.Row():
            with gr.Column():
                gr.Markdown("### è¾“å…¥å¸§")
                input_gallery = gr.Gallery(label="è¾“å…¥å¸§", show_label=False)
            with gr.Column():
                gr.Markdown("### è¶…åˆ†è¾¨ç‡å¸§")
                sr_gallery = gr.Gallery(label="è¶…åˆ†è¾¨ç‡å¸§", show_label=False)
        
        # åŸå§‹å¸§å¯¹æ¯”ï¼ˆå¦‚æœåˆ›å»ºäº†ä½åˆ†è¾¨ç‡ï¼‰
        with gr.Row():
            with gr.Column():
                gr.Markdown("### åŸå§‹é«˜åˆ†è¾¨ç‡å¸§ï¼ˆå¯¹æ¯”ç”¨ï¼‰")
                original_gallery = gr.Gallery(label="åŸå§‹å¸§", show_label=False)
        
        def process_video_wrapper(video_path, model_name, max_frames, create_lr, downscale_method):
            if video_path is None:
                return None, [], [], [], "è¯·å…ˆä¸Šä¼ è§†é¢‘"
            
            try:
                result = app.process_video_fixed(video_path, model_name, max_frames, create_lr, downscale_method)
                if create_lr:
                    output_path, input_frames, sr_frames, original_frames = result
                    status_msg = "å¤„ç†å®Œæˆï¼å·²åˆ›å»ºä½åˆ†è¾¨ç‡è¾“å…¥è¿›è¡Œè¶…åˆ†è¾¨ç‡æ¼”ç¤ºã€‚"
                    original_frames_to_show = original_frames if original_frames else []
                else:
                    output_path, input_frames, sr_frames, original_frames = result
                    status_msg = "å¤„ç†å®Œæˆï¼ç›´æ¥å¯¹åŸå§‹è§†é¢‘è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚"
                    original_frames_to_show = []
                
                return output_path, input_frames, sr_frames, original_frames_to_show, status_msg
            except Exception as e:
                import traceback
                traceback.print_exc()
                return None, [], [], [], f"å¤„ç†å¤±è´¥: {str(e)}"
        
        process_btn.click(
            process_video_wrapper,
            inputs=[video_input, model_selector, max_frames, create_lr_checkbox, downscale_method],
            outputs=[video_output, input_gallery, sr_gallery, original_gallery, status_text]
        )
        
        # æ·»åŠ è¯´æ˜
        with gr.Accordion("â„¹ï¸ ä¿®å¤è¯´æ˜", open=False):
            gr.Markdown("""
            ### ä¿®å¤çš„é—®é¢˜ï¼š
            
            1. **æ•°æ®èŒƒå›´é—®é¢˜**: ä½¿ç”¨ `data_utils.canonicalize()` æ­£ç¡®å½’ä¸€åŒ–æ•°æ®åˆ°[0,1]èŒƒå›´
            2. **æ•°æ®æ ¼å¼é—®é¢˜**: ç¡®ä¿è¾“å…¥tensoræ ¼å¼æ­£ç¡® (T, H, W, C)
            3. **æ¨¡å‹è¾“å…¥é—®é¢˜**: æŒ‰ç…§é¡¹ç›®è¦æ±‚å¤„ç†æ•°æ®æ ¼å¼
            
            ### ä¸»è¦ä¿®å¤ï¼š
            - âœ… ä½¿ç”¨æ­£ç¡®çš„æ•°æ®å½’ä¸€åŒ–å‡½æ•°
            - âœ… ç¡®ä¿æ•°æ®æ ¼å¼ç¬¦åˆæ¨¡å‹è¦æ±‚
            - âœ… æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            - âœ… æ”¹è¿›é”™è¯¯å¤„ç†
            
            ### ä½¿ç”¨å»ºè®®ï¼š
            - å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·è¿è¡Œ `python debug_vsr.py` è¿›è¡Œè¯Šæ–­
            - ç¡®ä¿é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶å­˜åœ¨ä¸”æ­£ç¡®
            - æ£€æŸ¥è¾“å…¥è§†é¢‘è´¨é‡
            """)
        
        gr.Markdown("---")
        gr.Markdown("åŸºäº [EGVSR-PyTorch](https://github.com/Thmen/EGVSR) é¡¹ç›®æ„å»º")
    
    return interface


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import gradio
        print("âœ… Gradio å·²å®‰è£…")
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… Gradio: pip install gradio")
        sys.exit(1)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_files = [
        "pretrained_models/EGVSR_iter420000.pth",
        "pretrained_models/TecoGAN_BD_iter500000.pth",
        "pretrained_models/FRVSR_BD_iter400000.pth"
    ]
    
    missing_files = []
    for file_path in model_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print("âš ï¸  è­¦å‘Š: ä»¥ä¸‹é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ç¼ºå¤±:")
        for file_path in missing_files:
            print(f"   - {file_path}")
        print("è¯·ä¸‹è½½ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶åˆ° pretrained_models ç›®å½•")
    
    # å¯åŠ¨åº”ç”¨
    interface = create_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        show_error=True
    ) 